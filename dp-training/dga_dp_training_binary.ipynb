{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68750,"status":"ok","timestamp":1714087349678,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"7rzq2JtILw3P","outputId":"32132e4d-3c78-4d6d-c86e-f861e02060f7"},"outputs":[],"source":["!pip install tensorflow-privacy\n","!pip install pyarrow\n","!pip install -q tensorflow-model-optimization\n","\n","import tensorflow_privacy as tp\n","import tensorflow as tf\n","import tensorflow_privacy\n","from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n","\n","import tensorflow_model_optimization as tfmot\n","\n","import pandas as pd\n","import numpy as np\n","import pyarrow\n","import pyarrow.parquet as pq\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","import matplotlib.pyplot as plt\n","import gc\n","import os\n","\n","dir='/content/drive/MyDrive/DGA_PPML/'\n","dir_model = dir + 'all_dp/'\n","dataset='all_dataset'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":522,"status":"ok","timestamp":1714087360161,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"bwdUxwaQDEXL"},"outputs":[],"source":["X_train = pd.read_parquet(dir+dataset+'/binary/X_train.parquet', engine='pyarrow')\n","X_test = pd.read_parquet(dir+dataset+'/binary/X_test.parquet', engine='pyarrow')\n","y_train = pd.read_parquet(dir+dataset+'/binary/y_train.parquet', engine='pyarrow')\n","y_test = pd.read_parquet(dir+dataset+'/binary/y_test.parquet', engine='pyarrow')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":650,"status":"ok","timestamp":1714089327327,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"Nn2G_2XWxRU8","outputId":"7485b3c2-35f9-43b5-b691-ba782b6b7810"},"outputs":[],"source":["# parameters\n","noise_multiplier = 0.1\n","epochs = 30\n","batch_size = 64\n","learning_rate = 0.001\n","num_microbatches = 1\n","l2_norm_clip = 1\n","n=np.shape(X_train)[0]\n","delta=1/n\n","noise_lbd=0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714087360162,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"WcF3PpySNEuH"},"outputs":[],"source":["# training binary model\n","def train_binary(epsilon, model, name_model):\n","\n","  print(model.summary())\n","\n","  hist = model.fit(X_train, y_train, batch_size=batch_size, validation_split=0.2, epochs=epochs, shuffle=True)\n","\n","  fig = plt.figure(figsize=(6, 3), dpi=150)\n","  plt.plot(hist.history['accuracy'], label='Training accuracy')\n","  plt.plot(hist.history['val_accuracy'], label='Validation accuracy')\n","  plt.title(f'Epoch x Accuracy model={name_model} eps={epsilon}')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Accuracy')\n","  plt.legend()\n","  fig.tight_layout()\n","  fig.savefig(dir_model+f\"accuracy_{name_model}_dp_eps_{epsilon}.png\", dpi=200)\n","\n","  fig = plt.figure(figsize=(6, 3), dpi=150)\n","  plt.plot(hist.history['accuracy'], label='Training accuracy')\n","  plt.title(f'Epoch x Accuracy model={name_model} eps={epsilon}')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Accuracy')\n","  plt.legend()\n","  fig.tight_layout()\n","  fig.savefig(dir_model+f\"accuracy1_{name_model}_dp_eps_{epsilon}.png\", dpi=200)\n","\n","  fig = plt.figure(figsize=(6, 3), dpi=150)\n","  plt.plot(hist.history['loss'], label='Training loss')\n","  plt.plot(hist.history['val_loss'], label='Validation loss')\n","  plt.title(f'Epoch x Loss model={name_model} eps={epsilon}')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.legend()\n","  plt.legend()\n","  fig.tight_layout()\n","  fig.savefig(dir_model+f\"loss_{name_model}_dp_eps_{epsilon}.png\", dpi=200)\n","\n","  fig = plt.figure(figsize=(6, 3), dpi=150)\n","  plt.plot(hist.history['loss'], label='Training loss')\n","  plt.title(f'Epoch x Loss model={name_model} eps={epsilon}')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.legend()\n","  plt.legend()\n","  fig.tight_layout()\n","  fig.savefig(dir_model+f\"loss1_{name_model}_dp_eps_{epsilon}.png\", dpi=200)\n","\n","  score_train, acc_train = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=1)\n","  score_test, acc_test = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n","\n","  print(f\"score_train: {score_train} acc_train: {acc_train}\")\n","  print(f\"score_test: {score_test} acc_test: {acc_test}\")\n","\n","  modelo_json = model.to_json()\n","  with open(dir_model+\"model_\"+name_model+\"_dp.json\", \"w\") as json_file:\n","      json_file.write(modelo_json)\n","  model.save_weights(dir_model+\"model_\"+name_model+\"_dp.h5\")\n","\n","  # Saving models to load on MPC protocols\n","  inputP0=dir_model+\"input-P0-0_\"+name_model+\"_embedding_dp\"\n","  os.system(\"echo \\\" \\\" > \"+ inputP0)\n","  i=0\n","  for w in model.get_weights():\n","    try:\n","      arq=dir_model+\"weights\"+str(i)+\".csv\"\n","      np.savetxt(arq, w.ravel(), delimiter=\" \",fmt='%f')\n","      os.system(\"cat \"+arq+\" >> \"+ inputP0)\n","      print(i, np.array(w).shape, np.ravel(w).shape)\n","      i=i+1\n","    except:\n","      print('erro',w.shape)\n","\n","  # Saving data to test inference in MPC protocols\n","  inputP1_aux=dir_model+\"input-P1-0_\"+name_model+\"_aux\"\n","  inputP1=dir_model+\"input-P1-0_\"+name_model+\"_onehotencoder_dp\"\n","  os.system(\"echo ''  > \"+ inputP1)\n","  os.system(\"echo '' > \"+ inputP1_aux)\n","  for i in range(30):\n","    one_hot_encoder_x=np.array(np.eye(128)[ X_test.iloc[i].values ])\n","    np.savetxt(inputP1_aux, np.ravel(one_hot_encoder_x), delimiter=\" \",fmt='%f')\n","    os.system(\"cat \"+inputP1_aux+\" >> \"+ inputP1)\n","    np.savetxt(inputP1_aux, y_test.iloc[i].values, delimiter=\" \",fmt='%f')\n","    os.system(\"cat \"+inputP1_aux+\" >> \"+ inputP1)\n","\n","  # Prediction\n","  predicao = model.predict(np.array(X_test).reshape(np.shape(X_test)[0],64))\n","  predicao = predicao>=0.5\n","\n","  print(classification_report(predicao, y_test))\n","  print(confusion_matrix(predicao, y_test))\n","\n","  # metrics quantization\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","  converter._experimental_lower_tensor_list_ops = False\n","  converter.target_spec.supported_types = [tf.float16]\n","  tflite_model = converter.convert()\n","\n","  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n","  interpreter.allocate_tensors()\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  predicao_quant=[]\n","  for i in range(len(X_test)):\n","      input_data = np.array(X_test.iloc[i], dtype=np.float32).reshape(1,64)\n","      interpreter.set_tensor(input_details[0][\"index\"], input_data)\n","      interpreter.invoke()\n","      prediction = interpreter.get_tensor(output_details[0][\"index\"])\n","      predicao_quant.append(prediction>=0.5)\n","\n","  print(classification_report(np.expand_dims(np.ravel(predicao_quant),1), y_test))\n","  print(confusion_matrix(np.expand_dims(np.ravel(predicao_quant),1), y_test))\n","\n","  return model\n","\n","def quantization(model, name_model):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","  converter._experimental_lower_tensor_list_ops = False\n","  converter.target_spec.supported_types = [tf.float16]\n","  tflite_model = converter.convert()\n","  open(dir_model+\"/\"+name_model+\"_quant.tflite\", \"wb\").write(tflite_model)\n","\n","  print(\"Original model in Mb:\", os.path.getsize(dir_model+\"model_\"+name_model+\"_dp.h5\") / float(2**20))\n","  print(\"Quantized model in Mb:\", os.path.getsize(dir_model+name_model+\"_quant.tflite\") / float(2**20))\n","  print(\"Compression ratio:\", os.path.getsize(dir_model+\"model_\"+name_model+\"_dp.h5\")/os.path.getsize(dir_model+\"/\"+name_model+\"_quant.tflite\"))\n","\n","\n","def get_accuracy_quantization(name_model):\n","  interpreter = tf.lite.Interpreter(model_path=dir_model+name_model+\"_quant.tflite\")\n","  interpreter.allocate_tensors()\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  predicao_quant=[]\n","  for i in range(len(X_test)):\n","    input_data = np.array(X_test.iloc[i], dtype=np.float32).reshape(1,64)\n","    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n","    interpreter.invoke()\n","    prediction = interpreter.get_tensor(output_details[0][\"index\"])\n","    predicao_quant.append(prediction>=0.5)\n","  return np.expand_dims(np.ravel(predicao_quant),1)\n","\n","def save_weigths_quantized(list_index_tensors_quantization, model, name_model):\n","  interpreter = tf.lite.Interpreter(model_path=dir_model+name_model+\"_quant.tflite\")\n","  interpreter.allocate_tensors()\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","\n","  inputP0=dir_model+\"input-P0-0_model_quantized_\"+name_model+\"_embedding_dp\"\n","  os.system(\"echo \\\" \\\" > \"+ inputP0)\n","  i=0\n","  j=0\n","  for i_tensor in list_index_tensors_quantization:\n","    try:\n","      w = interpreter.get_tensor(i_tensor)\n","      arq=dir_model+\"weights_quantized\"+str(i)+\".csv\"\n","      np.savetxt(arq, w.ravel(), delimiter=\" \",fmt='%.5f')\n","      os.system(\"cat \"+arq+\" >> \"+ inputP0)\n","      print(j, np.array(w).shape, np.ravel(w).shape)\n","      j+=1\n","    except:\n","      pass\n","\n","def save_weigths(model, name_model):\n","  inputP0=dir_model+\"input-P0-0_\"+name_model+\"_embedding_dp\"\n","  os.system(\"echo \\\" \\\" > \"+ inputP0)\n","  i=0\n","  for w in model.get_weights():\n","    try:\n","      arq=dir_model+\"weights\"+str(i)+\".csv\"\n","      np.savetxt(arq, w.ravel(), delimiter=\" \",fmt='%.8f')\n","      os.system(\"cat \"+arq+\" >> \"+ inputP0)\n","      print(i, np.array(w).shape, np.ravel(w).shape)\n","      i=i+1\n","    except:\n","      print('erro',w.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714087360162,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"HsRwI3SLrEy1"},"outputs":[],"source":["# DP-SGD\n","def runTraining_dpsgd(epsilon, model, name_model):\n","\n","  noise_multiplier = compute_noise(n=n, batch_size=batch_size,target_epsilon=epsilon, epochs=epochs, delta=delta, noise_lbd=noise_lbd)\n","\n","  optimizer = tp.DPKerasAdamOptimizer(l2_norm_clip=l2_norm_clip,\n","                                      noise_multiplier=noise_multiplier,\n","                                      num_microbatches=num_microbatches,\n","                                      learning_rate=learning_rate\n","                                      ,gradient_accumulation_steps=4)\n","\n","\n","  loss = tf.keras.losses.BinaryCrossentropy(axis=-1, reduction=tf.losses.Reduction.NONE, name='binary_crossentropy')\n","  model.compile(loss=loss,optimizer=optimizer, metrics=['accuracy'])\n","  tf.keras.utils.enable_interactive_logging()\n","  trained_model = train_binary(epsilon, model, name_model)\n","\n","\n","  return trained_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4246505,"status":"error","timestamp":1714010368801,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"9i0TygJfAQKP","outputId":"bf43014a-7fa8-47c6-e4c6-d8c20842baa4"},"outputs":[],"source":["# cnn1d_binary\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Embedding(input_dim=128, output_dim=128, input_length=64))\n","model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=2, activation = 'relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","name_model=\"cnn1d_binary\"\n","epsilons = [0.1, 2, 5]\n","\n","for epsilon in epsilons:\n","  model_trained = runTraining_dpsgd(epsilon, model, name_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":985},"executionInfo":{"elapsed":247076,"status":"error","timestamp":1694555697604,"user":{"displayName":"Ricardo Maia","userId":"03859582991370284512"},"user_tz":180},"id":"M90cvufW7ImI","outputId":"e39c0657-cfce-48b4-a340-6ae20d0492ee"},"outputs":[],"source":["# mlp_binary\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Embedding(input_dim=128, output_dim=128, input_length=64))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","name_model=\"mlp_binary\"\n","epsilons = [0.1, 2, 5]\n","for epsilon in epsilons:\n","  model = runTraining_dpsgd(epsilon, model, name_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pt9KrBEb7RYi"},"outputs":[],"source":["# lstm_binary\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Embedding(input_dim=128, output_dim=128, input_length=64))\n","model.add(tf.keras.layers.LSTM(units=32, return_sequences=True))\n","model.add(tf.keras.layers.LSTM(units=32))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(100, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","name_model=\"lstm_binary\"\n","epsilons = [0.1, 2, 5]\n","for epsilon in epsilons:\n","  model = runTraining_dpsgd(epsilon, model, name_model)"]}],"metadata":{"accelerator":"TPU","colab":{"gpuClass":"premium","gpuType":"V28","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}
